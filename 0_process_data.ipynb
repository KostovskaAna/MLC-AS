{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 61)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def prepare_metafeatures():\n",
    "    jsons = pd.read_json(\"./data/metafeatures.json\")\n",
    "    metafeatures = ['DefaultAccuracy', 'TotalDistinctClasses', 'UnseenInTrain', 'RatioTrainToPower', 'RatioTestToPower', \n",
    "    'RatioTotalToPower', 'RatioUnseenToTest', 'Attributes', 'Distinct labelsets', 'Instances', 'Labels', 'LxIxF', \n",
    "    'Ratio of number of instances to the number of attributes', 'Cardinality', 'Density', 'Maximal entropy of labels', 'Mean of entropies of labels', \n",
    "    'Minimal entropy of labels', 'Standard deviation of label cardinality', 'CVIR inter class', 'Kurtosis cardinality', 'Max IR inter class', 'Max IR intra class', \n",
    "    'Max IR per labelset', 'Mean of IR inter class', 'Mean of IR intra class', 'Mean of IR per labelset', 'Mean of standard deviation of IR intra class', \n",
    "    'Proportion of maxim label combination (PMax)', 'Proportion of unique label combination (PUniq)', 'Skewness cardinality', \n",
    "    'Average examples per labelset', 'Bound', 'Diversity', 'Number of labelsets up to 10 examples', 'Number of labelsets up to 2 examples', 'Number of labelsets up to 50 examples', \n",
    "    'Number of labelsets up to 5 examples', 'Mean examples per labelset', 'Number of unconditionally dependent label pairs by chi-square test', 'Proportion of distinct labelsets', \n",
    "    'Ratio of number of labelsets up to 10 examples', 'Ratio of number of labelsets up to 2 examples', 'Ratio of number of labelsets up to 50 examples', 'Ratio of number of labelsets up to 5 examples', \n",
    "    'Ratio of unconditionally dependent label pairs by chi-square test', 'SCUMBLE', 'Standard deviation of examples per labelset', 'Number of unique labelsets', \n",
    "    'Average absolute correlation between numeric attributes', 'Average gain ratio', 'Number of binary attributes', 'Mean of entropies of nominal attributes', 'Mean of kurtosis', \n",
    "    'Mean of mean of numeric attributes', 'Mean of skewness of numeric attributes', 'Mean of standard deviation of numeric attributes', 'Number of nominal attributes', \n",
    "    'Number of numeric attributes', 'Proportion of binary attributes', 'Proportion of nominal attributes', 'Proportion of numeric attributes', 'Proportion of numeric attributes with outliers']\n",
    "\n",
    "    colms = ['DATASET'] + metafeatures\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for index, row in jsons.iterrows():\n",
    "        dict_m = {}\n",
    "\n",
    "\n",
    "        for jsonElement in row['metafeatures']['train']:\n",
    "            k = list(jsonElement.keys())[0]\n",
    "            v = list(jsonElement.values())[0]['value']\n",
    "            dict_m[k] = v\n",
    "\n",
    "        arr = []\n",
    "        arr.append(row['name'])\n",
    "        for metafeature in metafeatures:\n",
    "            try:\n",
    "                val =  math.nan  if (dict_m[metafeature] == None) else dict_m[metafeature]\n",
    "            except:\n",
    "                val = math.nan  \n",
    "            arr.append(val)\n",
    "        data.append(arr)\n",
    "\n",
    "\n",
    "    df_mf = pd.DataFrame(data=data, columns=colms)\n",
    "\n",
    "\n",
    "    # remove constant value columns\n",
    "    df_mf = df_mf.loc[:, (df_mf != df_mf.iloc[0]).any()] \n",
    "    df_mf = df_mf.drop([\"RatioTrainToPower\",  \"RatioTestToPower\"], axis=1)\n",
    "    df_mf['DATASET'] = df_mf.apply(lambda row: row.DATASET.upper(), axis=1)\n",
    "\n",
    "\n",
    "    df_mf.set_index('DATASET', inplace=True)\n",
    "    # print(df_mf.columns.shape)\n",
    "\n",
    "    df_mf.to_csv('./data/metafeatures.csv')\n",
    "    print(df_mf.shape)\n",
    "\n",
    "prepare_metafeatures()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_metafeatures_and_regression_performance_data\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "metric, lower_is_better = 'HAMMING LOSS example based', True\n",
    "# metric, lower_is_better = 'MACRO F1', False\n",
    "# metric, lower_is_better = 'F1 example based', False\n",
    "# metric, lower_is_better = 'MICRO F1', False\n",
    "# metric, lower_is_better = 'AUCROC MICRO', False\n",
    "top_k = 5\n",
    "# lower_is_better = False\n",
    "\n",
    "# prepare metafeatures (they are always the same accross metrics and tasks) and regression performance data\n",
    "learning_task = 'regression'\n",
    "df_x, df_y, algo_portfolio, algo_counts = prepare_metafeatures_and_regression_performance_data(metric, lower_is_better, top_k)\n",
    "\n",
    "\n",
    "os.makedirs(f\"./processed_data/{metric}/regression/\", exist_ok=True) \n",
    "os.makedirs(f\"./processed_data/{metric}/classification/\", exist_ok=True) \n",
    "os.makedirs(f\"./processed_data/{metric}/pairwise_regression/\", exist_ok=True) \n",
    "os.makedirs(f\"./processed_data/{metric}/pairwise_classification/\", exist_ok=True) \n",
    "os.makedirs(f\"./processed_data/{metric}/cost_sensitive_pairwise_classification/\", exist_ok=True) \n",
    "\n",
    "df_x.to_csv(f\"./processed_data/metafeatures.csv\")\n",
    "df_y.to_csv(f\"./processed_data/{metric}/{learning_task}/performance.csv\")\n",
    "np.save(f\"./processed_data/{metric}/algo_portfolio.npy\", algo_portfolio, allow_pickle=True)\n",
    "np.save(f\"./processed_data/{metric}/algo_counts.npy\", algo_counts, allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "# based on the regression performance data, construct data for the other learning tasks\n",
    "df = pd.read_csv(f\"./processed_data/{metric}/{learning_task}/performance.csv\", index_col = 0)\n",
    "original_targets = df.columns\n",
    "\n",
    "\n",
    "# pairwise regression\n",
    "pairwise_combinations = list(combinations(original_targets, 2))\n",
    "for pair in pairwise_combinations:\n",
    "    target_name = f\"{pair[0]}_vs_{pair[1]}\"\n",
    "    df[target_name] = df[pair[0]] - df[pair[1]]  # Placeholder operation, adjust as necessary\n",
    "df_PR = df.iloc[:,top_k:]\n",
    "df_PR.to_csv(f\"./processed_data/{metric}/pairwise_regression/performance.csv\")\n",
    "\n",
    "\n",
    "# pairwise classification\n",
    "df = df.iloc[:,0:top_k]\n",
    "for pair in pairwise_combinations:\n",
    "    target_name = f\"{pair[0]}_vs_{pair[1]}\"\n",
    "    if lower_is_better:\n",
    "        # If lower values are better, 1 when the first algorithm outperforms the second\n",
    "        df[target_name] = [0 if df.loc[idx][pair[0]] < df.loc[idx][pair[1]] else 1 for idx in df.index]\n",
    "    else:\n",
    "        # If higher values are better, 1 when the first algorithm outperforms the second\n",
    "        df[target_name] = [0 if df.loc[idx][pair[0]] > df.loc[idx][pair[1]] else 1 for idx in df.index]\n",
    "df_PC = df.iloc[:,top_k:]\n",
    "df_PC.to_csv(f\"./processed_data/{metric}/pairwise_classification/performance.csv\")\n",
    "\n",
    "# classification\n",
    "df = df.iloc[:,0:top_k]\n",
    "best_algos = []\n",
    "for idx in df.index:\n",
    "    if lower_is_better:\n",
    "        best_algo = df.loc[idx, original_targets].idxmin()\n",
    "    else:\n",
    "        best_algo = df.loc[idx, original_targets].idxmax()\n",
    "    best_algos.append(best_algo)\n",
    "df['C'] = best_algos\n",
    "df[['C']].to_csv(f\"./processed_data/{metric}/classification/performance.csv\")\n",
    "\n",
    "# cost sensitive pairwise classification\n",
    "df = df.iloc[:,0:top_k]\n",
    "for pair in pairwise_combinations:\n",
    "    target_name = f\"{pair[0]}_vs_{pair[1]}\"\n",
    "    if lower_is_better:\n",
    "        # If lower values are better, 1 when the first algorithm outperforms the second\n",
    "        df[target_name] = [0 if df.loc[idx][pair[0]] < df.loc[idx][pair[1]] else 1 for idx in df.index]\n",
    "    else:\n",
    "        # If higher values are better, 1 when the first algorithm outperforms the second\n",
    "        df[target_name] = [0 if df.loc[idx][pair[0]] > df.loc[idx][pair[1]] else 1 for idx in df.index]\n",
    "    df[target_name+\"_cost\"] = [np.abs(df.loc[idx][pair[0]] - df.loc[idx][pair[1]]) for idx in df.index]\n",
    "df_CS_PC = df.iloc[:,top_k:]\n",
    "df_CS_PC.to_csv(f\"./processed_data/{metric}/cost_sensitive_pairwise_classification/performance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gecco2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
