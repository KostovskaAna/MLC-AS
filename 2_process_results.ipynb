{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_metrics(test_mse_for_output, test_mse_dummy_for_output, output_name, aggregated_results):\n",
    "    mean_test_mse = np.mean(test_mse_for_output)\n",
    "    std_test_mse = np.std(test_mse_for_output)\n",
    "    mean_test_mse_dummy = np.mean(test_mse_dummy_for_output)\n",
    "    std_test_mse_dummy = np.std(test_mse_dummy_for_output)\n",
    "    improvement = (mean_test_mse_dummy - mean_test_mse) / mean_test_mse_dummy * 100  # Improvement percentage\n",
    "    \n",
    "    aggregated_results['output'].append(output_name)\n",
    "    aggregated_results['mean_test_mse'].append(mean_test_mse)\n",
    "    aggregated_results['std_test_mse'].append(std_test_mse)\n",
    "    aggregated_results['mean_test_mse_dummy'].append(mean_test_mse_dummy)\n",
    "    aggregated_results['std_test_mse_dummy'].append(std_test_mse_dummy)\n",
    "    aggregated_results['improvement_over_dummy (%)'].append(improvement)\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "\n",
    "def performance_summary_regression(seed, metric, task_output):\n",
    "    output_names = np.load(f'./processed_data/{metric}/algo_portfolio.npy', allow_pickle=True)\n",
    "    num_outputs = len(output_names)\n",
    "\n",
    "    aggregated_results = {\n",
    "        'output': [],\n",
    "        'mean_test_mse': [],\n",
    "        'std_test_mse': [],\n",
    "        'mean_test_mse_dummy': [],\n",
    "        'std_test_mse_dummy': [],\n",
    "        'improvement_over_dummy (%)': []\n",
    "    }\n",
    "\n",
    "    if(task_output=='multi'):\n",
    "        results_df = pd.read_csv(f'./results/seed_{seed}/{metric}/regression/multi/performance_results.csv')\n",
    "        results_df['test_mse'] = results_df['test_mse'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "        results_df['test_mse_dummy'] = results_df['test_mse_dummy'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "        for i in range(num_outputs):\n",
    "            test_mse_for_output = np.array([mse[i] for mse in results_df['test_mse']])\n",
    "            test_mse_dummy_for_output = np.array([mse_dummy[i] for mse_dummy in results_df['test_mse_dummy']])\n",
    "            aggregated_results = process_metrics(test_mse_for_output, test_mse_dummy_for_output, output_names[i], aggregated_results)\n",
    "    elif(task_output=='single'):\n",
    "        for i in range(num_outputs):\n",
    "            results_df = pd.read_csv(f\"./results/seed_{seed}/{metric}/regression/single/performance_results_{output_names[i]}.csv\")\n",
    "            test_mse_for_output = np.array(results_df['test_mse'])\n",
    "            test_mse_dummy_for_output = np.array(results_df['test_mse_dummy'])\n",
    "            aggregated_results = process_metrics(test_mse_for_output, test_mse_dummy_for_output, output_names[i], aggregated_results)\n",
    "\n",
    "    return pd.DataFrame(aggregated_results)\n",
    "\n",
    "\n",
    "\n",
    "def performance_summary_classification(metric, task_output):\n",
    "    results_df = pd.read_csv(f'./results/seed_42/{metric}/classification/{task_output}/performance_results.csv', index_col=0)\n",
    "    mean_test_acc = np.mean(results_df['test_score'])\n",
    "    std_test_acc = np.std(results_df['test_score'])\n",
    "    mean_test_acc_dummy = np.mean(results_df['test_score_dummy'])\n",
    "    std_test_acc_dummy = np.std(results_df['test_score_dummy'])\n",
    "    improvement = (mean_test_acc - mean_test_acc_dummy) / mean_test_acc_dummy * 100  # Improvement percentage\n",
    "    aggregated_results = {\n",
    "        'output': ['C'],\n",
    "        'mean_test_acc': [mean_test_acc],\n",
    "        'std_test_acc': [std_test_acc],\n",
    "        'mean_test_acc_dummy': [mean_test_acc_dummy],\n",
    "        'std_test_acc_dummy': [std_test_acc_dummy],\n",
    "        'improvement_over_dummy (%)': [improvement]\n",
    "    }\n",
    "    return pd.DataFrame(aggregated_results)\n",
    "\n",
    "def performance_summary_pairwise_regression(metric, task_output):\n",
    "    aggregated_results = {\n",
    "        'output': [],\n",
    "        'mean_test_mse': [],\n",
    "        'std_test_mse': [],\n",
    "        'mean_test_mse_dummy': [],\n",
    "        'std_test_mse_dummy': [],\n",
    "        'improvement_over_dummy (%)': []\n",
    "    }\n",
    "    outputs = pd.read_csv(f'./processed_data/{metric}/pairwise_regression/performance.csv', index_col=0).columns\n",
    "    if task_output == 'multi':\n",
    "        results_df = pd.read_csv(f'./results/seed_42/{metric}/pairwise_regression/{task_output}/performance_results.csv', index_col=0)\n",
    "        results_df['test_mse'] = results_df['test_mse'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "        results_df['test_mse_dummy'] = results_df['test_mse_dummy'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "        for i in range(len(outputs)):\n",
    "            test_mse_for_output = np.array([mse[i] for mse in results_df['test_mse']])\n",
    "            test_mse_dummy_for_output = np.array([mse_dummy[i] for mse_dummy in results_df['test_mse_dummy']])\n",
    "            aggregated_results = process_metrics(test_mse_for_output, test_mse_dummy_for_output, outputs[i], aggregated_results)\n",
    "    elif task_output == 'single':\n",
    "        print('single')\n",
    "        for i in range(len(outputs)):\n",
    "            results_df = pd.read_csv(f\"./results/seed_42/{metric}/pairwise_regression/{task_output}/performance_results_{outputs[i]}.csv\")\n",
    "            test_mse_for_output = np.array(results_df['test_mse'])\n",
    "            test_mse_dummy_for_output = np.array(results_df['test_mse_dummy'])\n",
    "            aggregated_results = process_metrics(test_mse_for_output, test_mse_dummy_for_output, outputs[i], aggregated_results)\n",
    "\n",
    "    return pd.DataFrame(aggregated_results)\n",
    "\n",
    "def process_metrics_classification(test_score_for_output, test_score_dummy_for_output, output_name, aggregated_results):\n",
    "    mean_test_acc = np.mean(test_score_for_output)\n",
    "    std_test_acc = np.std(test_score_for_output)\n",
    "    mean_test_acc_dummy = np.mean(test_score_dummy_for_output)\n",
    "    std_test_acc_dummy = np.std(test_score_dummy_for_output)\n",
    "    improvement = (mean_test_acc - mean_test_acc_dummy) / mean_test_acc_dummy * 100  # Improvement percentage\n",
    "    \n",
    "    aggregated_results['output'].append(output_name)\n",
    "    aggregated_results['mean_test_acc'].append(mean_test_acc)\n",
    "    aggregated_results['std_test_acc'].append(std_test_acc)\n",
    "    aggregated_results['mean_test_acc_dummy'].append(mean_test_acc_dummy)\n",
    "    aggregated_results['std_test_acc_dummy'].append(std_test_acc_dummy)\n",
    "    aggregated_results['improvement_over_dummy (%)'].append(improvement)\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "def performance_summary_pairwise_classification(metric, task_output, cost_sensitive=False):\n",
    "    aggregated_results = {\n",
    "        'output': [],\n",
    "        'mean_test_acc': [],\n",
    "        'std_test_acc': [],\n",
    "        'mean_test_acc_dummy': [],\n",
    "        'std_test_acc_dummy': [],\n",
    "        'improvement_over_dummy (%)': []\n",
    "    }\n",
    "    outputs = pd.read_csv(f'./processed_data/{metric}/pairwise_classification/performance.csv', index_col=0).columns\n",
    "    if task_output == 'multi':\n",
    "        results_df = pd.read_csv(f'./results/seed_42/{metric}/pairwise_classification/{task_output}/performance_results.csv', index_col=0)\n",
    "        results_df['test_score'] = results_df['test_score'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
    "        results_df['test_score_dummy'] = results_df['test_score_dummy'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
    "        for i in range(len(outputs)):\n",
    "            test_mse_for_output = np.array([mse[i] for mse in results_df['test_score']])\n",
    "            test_mse_dummy_for_output = np.array([mse_dummy[i] for mse_dummy in results_df['test_score_dummy']])\n",
    "            aggregated_results = process_metrics_classification(test_mse_for_output, test_mse_dummy_for_output, outputs[i], aggregated_results)\n",
    "    elif task_output == 'single':\n",
    "        for i in range(len(outputs)):\n",
    "            results_df = pd.read_csv(f'./results/seed_42/{metric}/{\"cost_sensitive_\" if cost_sensitive else \"\"}pairwise_classification/{task_output}/performance_results_{outputs[i]}.csv')\n",
    "            test_mse_for_output = np.array(results_df['test_score'])\n",
    "            test_mse_dummy_for_output = np.array(results_df['test_score_dummy'])\n",
    "            aggregated_results = process_metrics_classification(test_mse_for_output, test_mse_dummy_for_output, outputs[i], aggregated_results)\n",
    "    return pd.DataFrame(aggregated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             output  mean_test_acc  std_test_acc  mean_test_acc_dummy  \\\n",
      "0    DEEP4_vs_RFPCT          0.725      0.446514                0.700   \n",
      "1       DEEP4_vs_CC          0.825      0.379967                0.750   \n",
      "2   DEEP4_vs_Ada300          0.575      0.494343                0.625   \n",
      "3   DEEP4_vs_TREMLC          0.750      0.433013                0.750   \n",
      "4       RFPCT_vs_CC          0.800      0.400000                0.675   \n",
      "5   RFPCT_vs_Ada300          0.600      0.489898                0.000   \n",
      "6   RFPCT_vs_TREMLC          0.575      0.494343                0.525   \n",
      "7      CC_vs_Ada300          0.700      0.458258                0.525   \n",
      "8      CC_vs_TREMLC          0.600      0.489898                0.725   \n",
      "9  Ada300_vs_TREMLC          0.550      0.497494                0.000   \n",
      "\n",
      "   std_test_acc_dummy  improvement_over_dummy (%)  \n",
      "0            0.458258                    3.571429  \n",
      "1            0.433013                   10.000000  \n",
      "2            0.484123                   -8.000000  \n",
      "3            0.433013                    0.000000  \n",
      "4            0.468375                   18.518519  \n",
      "5            0.000000                         inf  \n",
      "6            0.499375                    9.523810  \n",
      "7            0.499375                   33.333333  \n",
      "8            0.446514                  -17.241379  \n",
      "9            0.000000                         inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/2ybfk9216djb_4r6x147d9l40000gn/T/ipykernel_1045/3822348529.py:103: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  improvement = (mean_test_acc - mean_test_acc_dummy) / mean_test_acc_dummy * 100  # Improvement percentage\n"
     ]
    }
   ],
   "source": [
    "aggregated_results_df_pairwise_classification_multi = performance_summary_pairwise_classification('HAMMING LOSS example based', 'multi')\n",
    "print(aggregated_results_df_pairwise_classification_multi)\n",
    "\n",
    "# aggregated_results_df_pairwise_classification_single = performance_summary_pairwise_classification('HAMMING LOSS example based', 'single')\n",
    "# print(aggregated_results_df_pairwise_classification_single)\n",
    "\n",
    "# aggregated_results_df_pairwise_classification_single_cost_sensitive = performance_summary_pairwise_classification('HAMMING LOSS example based', 'single', True)\n",
    "# print(aggregated_results_df_pairwise_classification_single_cost_sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated_results_df_classification_single = performance_summary_classification('HAMMING LOSS example based', 'single')\n",
    "# print(aggregated_results_df_classification_single)\n",
    "\n",
    "# aggregated_results_df_multi = performance_summary_regression('HAMMING LOSS example based', 'multi')\n",
    "# print(aggregated_results_df_multi)\n",
    "\n",
    "# for seed in range(0, 10):\n",
    "#     print(\"seed: \", seed)\n",
    "#     aggregated_results_df_single = performance_summary_regression(seed, 'HAMMING LOSS example based', 'single')\n",
    "#     print(aggregated_results_df_single)\n",
    "\n",
    "# aggregated_results_df_pairwise_regression_single = performance_summary_pairwise_regression('HAMMING LOSS example based', 'single')\n",
    "# print(aggregated_results_df_pairwise_regression_single)\n",
    "\n",
    "# aggregated_results_df_pairwise_regression_multi = performance_summary_pairwise_regression('HAMMING LOSS example based', 'multi')\n",
    "# print(aggregated_results_df_pairwise_regression_multi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gecco2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
